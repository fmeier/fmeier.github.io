 I am a research scientist at <b>FAIR</b>@Meta AI in Menlo Park. I am the team lead of the Cortex team which is focussed on learning and evaluating foundation models for robotics. My research interests lie at the intersection of <b>machine learning</b> and <b>robotics</b>, with a special interest in lifelong learning. I have a Ph.D. from the <b>University of Southern California</b> and was advised by [Stefan Schaal](https://stefan-schaal.net). My publications are available on [my Google Scholar page](https://scholar.google.com/citations?user=7oxkHYYAAAAJ&hl=en) and my open source contributions can be found on [my Github profile](https://github.com/fmeier).


<b></b>
<b>Announcements</b>
<table class="table table-hover">
<tr>
<td class='col-md-3'> May 2025</td>
Our paper *Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D* was accepted at ICML 2025 as a **spotlight paper! (top 2.6%)**. Find more details on our <a href="https://locate3d.atmeta.com>">website</a> (with links to models, new dataset and demo).
</td>
<tr>
  <td class='col-md-3'>April 2025</td>
  <td> We have release Locate3D a new SOTA model for locating objects from natural language expressions. Locate3D runs directly on pointclouds constructed from a robots sensors! We've released the <a href="https://github.com/facebookresearch/locate-3d/">model</a> and a new human annotated dataset for **100K** 3D referring expressions</a>, and a demo <a href="https://locate3d.atmeta.com/demo">demo</a>. Learn more about the models capabilities on our <a href="https://locate3d.atmeta.com>">website</a> for more details.
</td>
</tr>
 <tr>
 </tr>
</table>



<b></b>
<b>Research Updates!</b>
<table class="table table-hover">
  <tr>
  <td class='col-md-3'>April 2024</td>
  <td>We’re releasing OpenEQA — the Open-Vocabulary Embodied Question Answering Benchmark. It measures an AI agent’s understanding of physical environments by probing it with open vocabulary questions like “Where did I leave my badge?”! See our <a href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/">blog post</a> and <a href="https://open-eqa.github.io">website</a> for more details.</td>
</tr>
 <tr>
  <td class='col-md-3'>May 2023</td>
  <td>We're presenting BC-IRL at ICLR 2023! <a href="https://arxiv.org/abs/2303.16194">paper</a>, <a href="https://github.com/facebookresearch/bc-irl">code</a>. In this work we analyze SOTA inverse reinforcement learning algorithms and show that reward functions are overfitting to the demonstrations used for training. We present an algorithm to train generalizable reward functions!</td>
</tr>
 <tr>
  <td class='col-md-3'>March 2023</td>
  <td>You can find VC-1 on Huggingface! <a href="https://huggingface.co/facebook/vc1-base">VC-1 Base</a> </td>
</tr>
 <tr>
  <td class='col-md-3'>March 2023</td>
  <td> We've open-sourced CortexBench and VC-1, find details on our <a href="https://eai-vc.github.io"> website </a>! In short, CortexBench is a benchmark to evaluate visual foundation models for robotics on 17 simulated robotics tasks, and VC-1, a visual foundation model that on average achieves SOTA on all 17 tasks! We summarize our findings on training VC-1 in our paper <a href="https://ai.meta.com/research/publications/where-are-we-in-the-search-for-an-artificial-visual-cortex-for-embodied-intelligence/">Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</a>, you can find the code <a href="https://github.com/facebookresearch/eai-vc/"> here </a> </td>
</tr>
 <tr>
  <td class='col-md-3'>January 2023</td>
  <td>Our Journal Paper on <a href="https://arxiv.org/abs/2011.03882">Multi-Modal Learning of Keypoint Predictive Models for Visual Object Manipulation</a> has been published in Transactions on Robotics. We leverage keypoint models to learn structured visual dynamics model for general object manipulation.</td>
</tr>
<tr>
  <td class='col-md-3'>January 2021</td>
  <td>Check out our blog post on <a href="https://ai.meta.com/blog/teaching-ai-to-manipulate-objects-using-visual-demos">Teaching AI to manipulate objects using visual demos</a> which highlights our work on learning from visual demonstrations via model-based (inverse) reinforcement learning</td>
</tr>
 <tr>
  <td class='col-md-3'>January 2021</td>
  <td>Our work on <a href="https://arxiv.org/abs/1906.05374">Meta-Learning via Learned Loss</a> has received the <b> best student paper award </b> at ICPR 2020</td>
</tr>
<tr>
  <td class='col-md-3'>October 2020</td>
  <td>Our work on <a href="https://arxiv.org/abs/2010.09034">Model-Based Inverse Reinforcement Learning from Visual Demonstrations</a> has been accepted at CoRL, <a href="https://sites.google.com/view/model-based-irl-from-vision"> website </a>, <a href="https://www.youtube.com/watch?v=sRrNhtLk12M&t"> video </a> </td>
</tr>
<tr>
  <td class='col-md-3'>October 2020</td>
  <td>We've open-sourced our library <a href="https://github.com/facebookresearch/differentiable-robot-model">Differentiable Robot Models</a> </td>
</tr>
<tr>
</tr>
</table>
<!-- ## <i class="fa fa-chevron-right"></i> Education -->

<!-- <table class="table table-hover">
  <tr>
    <td class="col-md-3">Aug 2014 - May 2019</td>
    <td>
        <strong>Ph.D. in Computer Science</strong>
          (0.00/0.00)
        <br>
      Carnegie Mellon University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2014 - May 2016</td>
    <td>
        <strong>M.S. in Computer Science</strong>
          (0.00/0.00)
        <br>
      Carnegie Mellon University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2011 - May 2014</td>
    <td>
        <strong>B.S. in Computer Science</strong>
          (3.99/4.00)
        <br>
      Virginia Tech
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2007 - May 2011</td>
    <td>
      Northside High School (Roanoke, Virginia)
    </td>
  </tr>
</table> -->


<!-- ## <i class="fa fa-chevron-right"></i> Experience
<table class="table table-hover">
<tr>
  <td class='col-md-3'>May 2019 - Present</td>
  <td><strong>Facebook AI</strong>, Research Scientist</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>June 2018 - Sept 2018</td>
  <td><strong>Intel Labs</strong>, Research Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2017 - Oct 2017</td>
  <td><strong>Google DeepMind</strong>, Research Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2014 - Aug 2014</td>
  <td><strong>Adobe Research</strong>, Data Scientist Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Dec 2013 - Jan 2014</td>
  <td><strong>Snowplow Analytics</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2013 - Aug 2013</td>
  <td><strong>Qualcomm</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2012 - Aug 2012</td>
  <td><strong>Phoenix Integration</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Jan 2011 - Aug 2011</td>
  <td><strong>Sunapsys</strong>, Network Administrator Intern</td>
</tr>
<tr>
</tr>
</table>

